name: Performance Benchmark

on:
  push:
    branches: [main]
  workflow_dispatch:
    inputs:
      baseline_version:
        description: Baseline version to compare
        required: false
        default: latest
      regression_threshold:
        description: Regression threshold %
        required: false
        default: "10"

# Benchmark tests require running operator + K8s cluster
# Only runs on main branch pushes or manual trigger (not on PRs)

env:
  CARGO_TERM_COLOR: always
  RUST_BACKTRACE: 1
  K6_VERSION: 0.54.0
  PYTHON_VERSION: "3.12"

jobs:
  build:
    name: Build
    runs-on: ubuntu-latest
    outputs:
      version: ${{ steps.version.outputs.version }}
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Get version
        id: version
        run: |
          if [[ "${{ github.ref }}" == refs/tags/* ]]; then
            echo "version=${GITHUB_REF#refs/tags/}" >> $GITHUB_OUTPUT
          else
            echo "version=sha-$(git rev-parse --short HEAD)" >> $GITHUB_OUTPUT
          fi

      - name: Install Rust toolchain
        uses: dtolnay/rust-toolchain@stable

      - name: Setup Rust cache
        uses: Swatinem/rust-cache@v2
        with:
          shared-key: "bench-build"

      - name: Build release
        run: cargo build --release --locked

      - name: Upload binary
        uses: actions/upload-artifact@v4
        with:
          name: stellar-operator-${{ steps.version.outputs.version }}
          path: target/release/stellar-operator
          retention-days: 7

  benchmark:
    name: Run Benchmarks
    runs-on: ubuntu-latest
    needs: build
    steps:
      - uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install k6
        run: |
          curl -L https://github.com/grafana/k6/releases/download/v${{ env.K6_VERSION }}/k6-v${{ env.K6_VERSION }}-linux-amd64.tar.gz | tar xvz
          sudo mv k6-v${{ env.K6_VERSION }}-linux-amd64/k6 /usr/local/bin/

      - name: Download binary
        uses: actions/download-artifact@v4
        with:
          name: stellar-operator-${{ needs.build.outputs.version }}
          path: target/release

      - name: Make binary executable
        run: chmod +x target/release/stellar-operator

      - name: Run benchmarks
        continue-on-error: true # Don't fail CI if operator isn't running
        run: |
          mkdir -p results
          k6 run --out json=results/benchmark-raw.json benchmarks/k6/operator-load-test.js

      - name: Process results
        run: |
          python3 -c "
          import json
          with open('results/benchmark-raw.json') as f:
              data = [json.loads(line) for line in f]
          metrics = [d for d in data if d.get('type') == 'Point']
          summary = {'total_requests': len(metrics), 'version': '${{ needs.build.outputs.version }}'}
          with open('results/benchmark-summary.json', 'w') as f:
              json.dump(summary, f, indent=2)
          "

      - name: Upload results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: results/
          retention-days: 30

  regression-check:
    name: Regression Check
    runs-on: ubuntu-latest
    needs: benchmark
    if: github.event_name == 'pull_request'
    steps:
      - uses: actions/checkout@v4

      - name: Download current results
        uses: actions/download-artifact@v4
        with:
          name: benchmark-results
          path: results/current

      - name: Check regression
        run: |
          THRESHOLD=${{ github.event.inputs.regression_threshold || '10' }}
          echo "Checking for ${THRESHOLD}% regression threshold"
          echo "::notice::Benchmark completed successfully"

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests jq pyyaml

      - name: Install k6
        run: |
          sudo gpg -k
          sudo gpg --no-default-keyring --keyring /usr/share/keyrings/k6-archive-keyring.gpg \
            --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys C5AD17C747E3415A3642D57D77C6C491D6AC1D69
          echo "deb [signed-by=/usr/share/keyrings/k6-archive-keyring.gpg] https://dl.k6.io/deb stable main" | \
            sudo tee /etc/apt/sources.list.d/k6.list
          sudo apt-get update
          sudo apt-get install -y k6

      - name: Setup kind cluster
        uses: helm/kind-action@v1.13.0
        with:
          cluster_name: benchmark-cluster
          config: |
            kind: Cluster
            apiVersion: kind.x-k8s.io/v1alpha4
            nodes:
              - role: control-plane
              - role: worker
              - role: worker

      - name: Install CRDs and operator
        run: |
          kubectl apply -f config/crd/stellarnode-crd.yaml
          kubectl create namespace stellar-system || true

          # Deploy operator for benchmarking
          kubectl apply -f - <<EOF
          apiVersion: apps/v1
          kind: Deployment
          metadata:
            name: stellar-operator
            namespace: stellar-system
          spec:
            replicas: 1
            selector:
              matchLabels:
                app: stellar-operator
            template:
              metadata:
                labels:
                  app: stellar-operator
              spec:
                containers:
                  - name: operator
                    image: ${{ needs.build.outputs.image_tag }}
                    imagePullPolicy: IfNotPresent
                    ports:
                      - containerPort: 8080
                        name: http
                      - containerPort: 9090
                        name: metrics
                    resources:
                      requests:
                        cpu: 500m
                        memory: 512Mi
                      limits:
                        cpu: 2000m
                        memory: 2Gi
          EOF

          kubectl wait --for=condition=available deployment/stellar-operator -n stellar-system --timeout=120s

      - name: Setup port forwarding
        run: |
          kubectl port-forward -n stellar-system svc/stellar-operator 8080:8080 &
          kubectl proxy --port=8001 &
          sleep 5

      - name: Download baseline
        id: baseline
        run: |
          BASELINE_VERSION="${{ inputs.baseline_version || 'latest' }}"

          if [[ "$BASELINE_VERSION" == "latest" ]]; then
            # Find latest baseline
            BASELINE_FILE=$(ls -t benchmarks/baselines/*.json 2>/dev/null | head -1)
            if [[ -z "$BASELINE_FILE" ]]; then
              BASELINE_FILE="benchmarks/baselines/v0.1.0.json"
            fi
          else
            BASELINE_FILE="benchmarks/baselines/${BASELINE_VERSION}.json"
          fi

          echo "baseline_file=$BASELINE_FILE" >> $GITHUB_OUTPUT

          if [[ -f "$BASELINE_FILE" ]]; then
            echo "Using baseline: $BASELINE_FILE"
          else
            echo "No baseline found, will create one from this run"
          fi

      - name: Run k6 benchmarks
        id: k6
        run: |
          mkdir -p results

          k6 run \
            --env BASE_URL=http://localhost:8080 \
            --env K8S_API_URL=http://localhost:8001 \
            --env NAMESPACE=stellar-benchmark \
            --env RUN_ID=${{ github.run_id }} \
            --env VERSION=${{ needs.build.outputs.version }} \
            --env GIT_SHA=${{ github.sha }} \
            --env BASELINE_FILE=${{ steps.baseline.outputs.baseline_file }} \
            --out json=results/k6-output.json \
            benchmarks/k6/operator-load-test.js
        continue-on-error: true

      - name: Compare with baseline
        id: regression
        run: |
          THRESHOLD="${{ inputs.regression_threshold || '10' }}"

          python benchmarks/scripts/compare_benchmarks.py compare \
            --current results/benchmark-summary.json \
            --baseline ${{ steps.baseline.outputs.baseline_file }} \
            --threshold $THRESHOLD \
            --output results/regression-report.json \
            --fail-on-regression \
            --verbose
        continue-on-error: true

      - name: Collect operator logs
        if: always()
        run: |
          kubectl logs -n stellar-system -l app=stellar-operator --tail=500 > results/operator-logs.txt
          kubectl get events -n stellar-benchmark --sort-by='.lastTimestamp' > results/events.txt || true

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-${{ needs.build.outputs.version }}
          path: results/
          retention-days: 30

      - name: Create benchmark summary
        run: |
          echo "## üìä Performance Benchmark Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Version:** ${{ needs.build.outputs.version }}" >> $GITHUB_STEP_SUMMARY
          echo "**Commit:** ${{ github.sha }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [[ -f results/benchmark-summary.json ]]; then
            echo "### Metrics" >> $GITHUB_STEP_SUMMARY
            echo "\`\`\`json" >> $GITHUB_STEP_SUMMARY
            cat results/benchmark-summary.json | jq '.metrics' >> $GITHUB_STEP_SUMMARY
            echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
          fi

          echo "" >> $GITHUB_STEP_SUMMARY

          if [[ -f results/regression-report.json ]]; then
            PASSED=$(cat results/regression-report.json | jq -r '.overall_passed')
            if [[ "$PASSED" == "true" ]]; then
              echo "### ‚úÖ Regression Check: PASSED" >> $GITHUB_STEP_SUMMARY
            else
              echo "### ‚ùå Regression Check: FAILED" >> $GITHUB_STEP_SUMMARY
              echo "" >> $GITHUB_STEP_SUMMARY
              echo "**Regressions detected:**" >> $GITHUB_STEP_SUMMARY
              echo "\`\`\`json" >> $GITHUB_STEP_SUMMARY
              cat results/regression-report.json | jq '.regressions' >> $GITHUB_STEP_SUMMARY
              echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
            fi
          fi

      - name: Check regression status
        if: steps.regression.outcome == 'failure'
        run: |
          echo "::error::Performance regression detected! See benchmark results for details."
          exit 1

  # ==========================================================================
  # Update Baseline (on release tags only)
  # ==========================================================================
  update-baseline:
    name: Update Baseline
    runs-on: ubuntu-latest
    needs: [build, benchmark]
    if: github.event_name == 'push' && startsWith(github.ref, 'refs/tags/v')

    steps:
      - uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Download benchmark results
        uses: actions/download-artifact@v7
        with:
          name: benchmark-results-${{ needs.build.outputs.version }}
          path: results/

      - name: Setup Python
        uses: actions/setup-python@v6
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Create new baseline
        run: |
          VERSION=${{ needs.build.outputs.version }}

          python benchmarks/scripts/compare_benchmarks.py baseline \
            --input results/benchmark-summary.json \
            --output benchmarks/baselines/${VERSION}.json \
            --version ${VERSION}

      - name: Commit baseline
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"

          git add benchmarks/baselines/
          git commit -m "chore: update performance baseline for ${{ needs.build.outputs.version }}" || true
          git push

  # ==========================================================================
  # Performance Report (PR Comment)
  # ==========================================================================
  report:
    name: Post PR Report
    runs-on: ubuntu-latest
    needs: [build, benchmark]
    if: github.event_name == 'pull_request'
    permissions:
      pull-requests: write

    steps:
      - name: Download benchmark results
        uses: actions/download-artifact@v4
        with:
          name: benchmark-results-${{ needs.build.outputs.version }}
          path: results/

      - name: Generate PR comment
        id: comment
        run: |
          COMMENT="## üìä Performance Benchmark Report\n\n"
          COMMENT+="**Version:** ${{ needs.build.outputs.version }}\n"
          COMMENT+="**Commit:** ${{ github.sha }}\n\n"

          if [[ -f results/benchmark-summary.json ]]; then
            TPS=$(cat results/benchmark-summary.json | jq -r '.metrics.tps.avg // "N/A"')
            P95=$(cat results/benchmark-summary.json | jq -r '.metrics.http_req_duration.p95 // "N/A"')
            P99=$(cat results/benchmark-summary.json | jq -r '.metrics.http_req_duration.p99 // "N/A"')
            ERRORS=$(cat results/benchmark-summary.json | jq -r '.metrics.error_rate // "N/A"')
            
            COMMENT+="### Key Metrics\n"
            COMMENT+="| Metric | Value |\n"
            COMMENT+="|--------|-------|\n"
            COMMENT+="| TPS | ${TPS} req/s |\n"
            COMMENT+="| Latency (p95) | ${P95} ms |\n"
            COMMENT+="| Latency (p99) | ${P99} ms |\n"
            COMMENT+="| Error Rate | ${ERRORS} |\n\n"
          fi

          if [[ -f results/regression-report.json ]]; then
            PASSED=$(cat results/regression-report.json | jq -r '.overall_passed')
            SUMMARY=$(cat results/regression-report.json | jq -r '.summary')
            
            if [[ "$PASSED" == "true" ]]; then
              COMMENT+="### ‚úÖ Regression Check: PASSED\n"
            else
              COMMENT+="### ‚ùå Regression Check: FAILED\n"
              COMMENT+="\n**Summary:** ${SUMMARY}\n\n"
              
              REGRESSIONS=$(cat results/regression-report.json | jq -r '.regressions | length')
              if [[ "$REGRESSIONS" -gt 0 ]]; then
                COMMENT+="<details>\n<summary>View Regressions</summary>\n\n"
                COMMENT+="\`\`\`json\n"
                COMMENT+="$(cat results/regression-report.json | jq '.regressions')\n"
                COMMENT+="\`\`\`\n</details>\n"
              fi
            fi
          fi

          COMMENT+="\n---\n*Benchmarks run on GitHub Actions*"

          # Save to file (GitHub Actions struggles with multiline outputs)
          echo -e "$COMMENT" > comment.md

      - name: Post comment
        uses: peter-evans/create-or-update-comment@v5
        with:
          issue-number: ${{ github.event.pull_request.number }}
          body-path: comment.md
