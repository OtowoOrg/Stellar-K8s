name: Performance Benchmark

on:
  push:
    branches: [main, develop]
    tags: ['v*']
  pull_request:
    branches: [main]
  workflow_dispatch:
    inputs:
      baseline_version:
        description: 'Baseline version to compare against'
        required: false
        default: 'latest'
      regression_threshold:
        description: 'Regression threshold percentage'
        required: false
        default: '10'

env:
  CARGO_TERM_COLOR: always
  K6_VERSION: '0.49.0'
  PYTHON_VERSION: '3.11'

jobs:
  # ==========================================================================
  # Build Operator for Benchmarking
  # ==========================================================================
  build:
    name: Build Operator
    runs-on: ubuntu-latest
    outputs:
      version: ${{ steps.version.outputs.version }}
      image_tag: ${{ steps.version.outputs.image_tag }}
    
    steps:
      - uses: actions/checkout@v6
        with:
          fetch-depth: 0
      
      - name: Convert repository to lowercase
        id: repo-lower
        run: echo "repo_lc=$(echo ${{ github.repository }} | tr '[:upper:]' '[:lower:]')" >> $GITHUB_OUTPUT
      
      - name: Determine version
        id: version
        run: |
          if [[ "${{ github.ref }}" == refs/tags/* ]]; then
            VERSION=${GITHUB_REF#refs/tags/}
          else
            VERSION="sha-$(git rev-parse --short HEAD)"
          fi
          echo "version=$VERSION" >> $GITHUB_OUTPUT
          echo "image_tag=ghcr.io/${{ steps.repo-lower.outputs.repo_lc }}:benchmark-$VERSION" >> $GITHUB_OUTPUT
      
      - name: Install Rust toolchain
        uses: dtolnay/rust-toolchain@stable
      
      - name: Cache cargo
        uses: actions/cache@v5
        with:
          path: |
            ~/.cargo/bin
            ~/.cargo/registry/index
            ~/.cargo/registry/cache
            ~/.cargo/git/db
            target
          key: ${{ runner.os }}-cargo-bench-${{ hashFiles('**/Cargo.lock') }}
          restore-keys: |
            ${{ runner.os }}-cargo-bench-
            ${{ runner.os }}-cargo-
      
      - name: Build release binary
        run: cargo build --release
      
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
      
      - name: Build Docker image
        uses: docker/build-push-action@v6
        with:
          context: .
          push: false
          load: true
          tags: ${{ steps.version.outputs.image_tag }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
      
      - name: Save Docker image
        run: |
          mkdir -p /tmp/docker-images
          docker save ${{ steps.version.outputs.image_tag }} -o /tmp/docker-images/operator-image.tar
          ls -lh /tmp/docker-images/operator-image.tar
      
      - name: Upload Docker image artifact
        uses: actions/upload-artifact@v6
        with:
          name: operator-docker-image
          path: /tmp/docker-images/operator-image.tar
          retention-days: 1

  # ==========================================================================
  # Performance Benchmark Suite
  # ==========================================================================
  benchmark:
    name: Run Benchmarks
    runs-on: ubuntu-latest
    needs: build
    
    steps:
      - uses: actions/checkout@v6
      
      - name: Setup Python
        uses: actions/setup-python@v6
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests jq pyyaml
      
      - name: Install k6
        run: |
          sudo gpg -k
          sudo gpg --no-default-keyring --keyring /usr/share/keyrings/k6-archive-keyring.gpg \
            --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys C5AD17C747E3415A3642D57D77C6C491D6AC1D69
          echo "deb [signed-by=/usr/share/keyrings/k6-archive-keyring.gpg] https://dl.k6.io/deb stable main" | \
            sudo tee /etc/apt/sources.list.d/k6.list
          sudo apt-get update
          sudo apt-get install -y k6
      
      - name: Setup kind cluster
        uses: helm/kind-action@v1.13.0
        with:
          cluster_name: benchmark-cluster
          config: .github/kind-config.yaml
          wait: 300s
      
      - name: Download operator image artifact
        uses: actions/download-artifact@v6
        with:
          name: operator-docker-image
          path: /tmp/docker-images
      
      - name: Load operator image into kind
        run: |
          echo "Loading operator image from artifact..."
          kind load image-archive /tmp/docker-images/operator-image.tar --name benchmark-cluster
          echo "‚úì Image loaded successfully"
      
      - name: Wait for cluster to be ready
        run: |
          echo "Waiting for cluster nodes..."
          kubectl wait --for=condition=Ready node --all --timeout=120s || true
          
          echo "Checking cluster status..."
          kubectl cluster-info
          kubectl get nodes -o wide
      
      - name: Install CRDs and operator
        run: |
          kubectl apply -f config/crd/stellarnode-crd.yaml
          kubectl create namespace stellar-system || true
          
          # Deploy operator for benchmarking
          kubectl apply -f - <<EOF
          apiVersion: apps/v1
          kind: Deployment
          metadata:
            name: stellar-operator
            namespace: stellar-system
          spec:
            replicas: 1
            selector:
              matchLabels:
                app: stellar-operator
            template:
              metadata:
                labels:
                  app: stellar-operator
              spec:
                containers:
                  - name: operator
                    image: ${{ needs.build.outputs.image_tag }}
                    imagePullPolicy: IfNotPresent
                    ports:
                      - containerPort: 8080
                        name: http
                      - containerPort: 9090
                        name: metrics
                    resources:
                      requests:
                        cpu: 500m
                        memory: 512Mi
                      limits:
                        cpu: 2000m
                        memory: 2Gi
          EOF
          
          echo "Waiting for operator deployment to be available..."
          if ! kubectl wait --for=condition=available deployment/stellar-operator -n stellar-system --timeout=120s; then
            echo "‚ö†Ô∏è Deployment did not become available in time. Debugging info:"
            echo "Pod status:"
            kubectl get pods -n stellar-system -o wide
            echo "Pod logs:"
            kubectl logs -n stellar-system -l app=stellar-operator --tail=50 || true
            echo "Pod events:"
            kubectl describe pods -n stellar-system -l app=stellar-operator || true
            exit 1
          fi
          
          echo "‚úì Operator deployment is available"
      
      - name: Setup port forwarding
        run: |
          # Create a script to maintain port forwarding
          cat > /tmp/maintain-portforward.sh << 'SCRIPT'
          #!/bin/bash
          set -e
          
          # Start port forwarding and keep it alive
          while true; do
            kubectl port-forward -n stellar-system svc/stellar-operator 8080:8080 2>&1 || true
            sleep 2
          done
          SCRIPT
          
          chmod +x /tmp/maintain-portforward.sh
          
          # Start the port forwarding maintenance in background
          /tmp/maintain-portforward.sh > /tmp/pf-8080.log 2>&1 &
          echo $! > /tmp/pf-8080.pid
          
          # Wait for port-forward to be ready
          sleep 3
          
          # Verify connectivity
          for i in {1..30}; do
            if curl -s http://localhost:8080/api/v1/namespaces > /dev/null 2>&1; then
              echo "‚úì Operator API is accessible on port 8080"
              break
            fi
            if [ $i -eq 30 ]; then
              echo "‚ö†Ô∏è Warning: Operator API not accessible on port 8080"
              echo "Port forward log:"
              cat /tmp/pf-8080.log || true
            fi
            sleep 1
          done
      
      - name: Download baseline
        id: baseline
        run: |
          BASELINE_VERSION="${{ inputs.baseline_version || 'latest' }}"
          
          if [[ "$BASELINE_VERSION" == "latest" ]]; then
            # Find latest baseline
            BASELINE_FILE=$(ls -t benchmarks/baselines/*.json 2>/dev/null | head -1)
            if [[ -z "$BASELINE_FILE" ]]; then
              BASELINE_FILE="benchmarks/baselines/v0.1.0.json"
            fi
          else
            BASELINE_FILE="benchmarks/baselines/${BASELINE_VERSION}.json"
          fi
          
          echo "baseline_file=$BASELINE_FILE" >> $GITHUB_OUTPUT
          
          if [[ -f "$BASELINE_FILE" ]]; then
            echo "Using baseline: $BASELINE_FILE"
          else
            echo "No baseline found, will create one from this run"
          fi
      
      - name: Run k6 benchmarks
        id: k6
        run: |
          mkdir -p results
          
          # Verify connectivity before running benchmarks
          echo "Checking operator connectivity..."
          for i in {1..30}; do
            if curl -s http://localhost:8080/api/v1/namespaces > /dev/null 2>&1; then
              echo "‚úì Operator is accessible"
              break
            fi
            echo "Attempt $i/30: Waiting for operator..."
            sleep 1
          done
          
          # Use kubectl directly for API URLs (no proxy needed)
          k6 run \
            --env BASE_URL=http://localhost:8080 \
            --env NAMESPACE=stellar-benchmark \
            --env RUN_ID=${{ github.run_id }} \
            --env VERSION=${{ needs.build.outputs.version }} \
            --env GIT_SHA=${{ github.sha }} \
            --env BASELINE_FILE=${{ steps.baseline.outputs.baseline_file }} \
            --out json=results/k6-output.json \
            benchmarks/k6/operator-load-test.js
        continue-on-error: true
      
      - name: Compare with baseline
        id: regression
        run: |
          THRESHOLD="${{ inputs.regression_threshold || '10' }}"
          
          python benchmarks/scripts/compare_benchmarks.py compare \
            --current results/benchmark-summary.json \
            --baseline ${{ steps.baseline.outputs.baseline_file }} \
            --threshold $THRESHOLD \
            --output results/regression-report.json \
            --fail-on-regression \
            --verbose
        continue-on-error: true
      
      - name: Collect operator logs
        if: always()
        run: |
          mkdir -p results
          
          echo "Collecting operator logs..."
          kubectl logs -n stellar-system -l app=stellar-operator --tail=500 > results/operator-logs.txt 2>&1 || echo "Could not collect operator logs"
          
          echo "Collecting benchmark namespace events..."
          kubectl get events -n stellar-benchmark --sort-by='.lastTimestamp' > results/events.txt 2>&1 || echo "No events found in benchmark namespace"
          
          echo "Collecting operator pod details..."
          kubectl describe pods -n stellar-system -l app=stellar-operator > results/operator-pod-details.txt 2>&1 || echo "Could not describe operator pods"
      
      - name: Upload benchmark results
        uses: actions/upload-artifact@v6
        with:
          name: benchmark-results-${{ needs.build.outputs.version }}
          path: results/
          retention-days: 30
      
      - name: Create benchmark summary
        run: |
          echo "## üìä Performance Benchmark Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Version:** ${{ needs.build.outputs.version }}" >> $GITHUB_STEP_SUMMARY
          echo "**Commit:** ${{ github.sha }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [[ -f results/benchmark-summary.json ]]; then
            echo "### Metrics" >> $GITHUB_STEP_SUMMARY
            echo "\`\`\`json" >> $GITHUB_STEP_SUMMARY
            cat results/benchmark-summary.json | jq '.metrics' >> $GITHUB_STEP_SUMMARY
            echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [[ -f results/regression-report.json ]]; then
            PASSED=$(cat results/regression-report.json | jq -r '.overall_passed')
            if [[ "$PASSED" == "true" ]]; then
              echo "### ‚úÖ Regression Check: PASSED" >> $GITHUB_STEP_SUMMARY
            else
              echo "### ‚ùå Regression Check: FAILED" >> $GITHUB_STEP_SUMMARY
              echo "" >> $GITHUB_STEP_SUMMARY
              echo "**Regressions detected:**" >> $GITHUB_STEP_SUMMARY
              echo "\`\`\`json" >> $GITHUB_STEP_SUMMARY
              cat results/regression-report.json | jq '.regressions' >> $GITHUB_STEP_SUMMARY
              echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
            fi
          fi
      
      - name: Check regression status
        if: steps.regression.outcome == 'failure'
        run: |
          echo "::error::Performance regression detected! See benchmark results for details."
          exit 1

  # ==========================================================================
  # Update Baseline (on release tags only)
  # ==========================================================================
  update-baseline:
    name: Update Baseline
    runs-on: ubuntu-latest
    needs: [build, benchmark]
    if: github.event_name == 'push' && startsWith(github.ref, 'refs/tags/v')
    
    steps:
      - uses: actions/checkout@v6
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
      
      - name: Download benchmark results
        uses: actions/download-artifact@v7
        with:
          name: benchmark-results-${{ needs.build.outputs.version }}
          path: results/
      
      - name: Setup Python
        uses: actions/setup-python@v6
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Create new baseline
        run: |
          VERSION=${{ needs.build.outputs.version }}
          
          python benchmarks/scripts/compare_benchmarks.py baseline \
            --input results/benchmark-summary.json \
            --output benchmarks/baselines/${VERSION}.json \
            --version ${VERSION}
      
      - name: Commit baseline
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          
          git add benchmarks/baselines/
          git commit -m "chore: update performance baseline for ${{ needs.build.outputs.version }}" || true
          git push

  # ==========================================================================
  # Performance Report (PR Comment)
  # ==========================================================================
  report:
    name: Post PR Report
    runs-on: ubuntu-latest
    needs: [build, benchmark]
    if: github.event_name == 'pull_request'
    permissions:
      pull-requests: write
    
    steps:
      - name: Download benchmark results
        uses: actions/download-artifact@v7
        with:
          name: benchmark-results-${{ needs.build.outputs.version }}
          path: results/
      
      - name: Generate PR comment
        id: comment
        run: |
          COMMENT="## üìä Performance Benchmark Report\n\n"
          COMMENT+="**Version:** ${{ needs.build.outputs.version }}\n"
          COMMENT+="**Commit:** ${{ github.sha }}\n\n"
          
          if [[ -f results/benchmark-summary.json ]]; then
            TPS=$(cat results/benchmark-summary.json | jq -r '.metrics.tps.avg // "N/A"')
            P95=$(cat results/benchmark-summary.json | jq -r '.metrics.http_req_duration.p95 // "N/A"')
            P99=$(cat results/benchmark-summary.json | jq -r '.metrics.http_req_duration.p99 // "N/A"')
            ERRORS=$(cat results/benchmark-summary.json | jq -r '.metrics.error_rate // "N/A"')
            
            COMMENT+="### Key Metrics\n"
            COMMENT+="| Metric | Value |\n"
            COMMENT+="|--------|-------|\n"
            COMMENT+="| TPS | ${TPS} req/s |\n"
            COMMENT+="| Latency (p95) | ${P95} ms |\n"
            COMMENT+="| Latency (p99) | ${P99} ms |\n"
            COMMENT+="| Error Rate | ${ERRORS} |\n\n"
          fi
          
          if [[ -f results/regression-report.json ]]; then
            PASSED=$(cat results/regression-report.json | jq -r '.overall_passed')
            SUMMARY=$(cat results/regression-report.json | jq -r '.summary')
            
            if [[ "$PASSED" == "true" ]]; then
              COMMENT+="### ‚úÖ Regression Check: PASSED\n"
            else
              COMMENT+="### ‚ùå Regression Check: FAILED\n"
              COMMENT+="\n**Summary:** ${SUMMARY}\n\n"
              
              REGRESSIONS=$(cat results/regression-report.json | jq -r '.regressions | length')
              if [[ "$REGRESSIONS" -gt 0 ]]; then
                COMMENT+="<details>\n<summary>View Regressions</summary>\n\n"
                COMMENT+="\`\`\`json\n"
                COMMENT+="$(cat results/regression-report.json | jq '.regressions')\n"
                COMMENT+="\`\`\`\n</details>\n"
              fi
            fi
          fi
          
          COMMENT+="\n---\n*Benchmarks run on GitHub Actions*"
          
          # Save to file (GitHub Actions struggles with multiline outputs)
          echo -e "$COMMENT" > comment.md
      
      - name: Post comment
        uses: peter-evans/create-or-update-comment@v5
        with:
          issue-number: ${{ github.event.pull_request.number }}
          body-path: comment.md